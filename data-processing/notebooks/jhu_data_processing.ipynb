{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process JHU Data\n",
    "\n",
    "This notebook downloads the latest JHU data from GitHub, processes it for dashboard visualization, and places it in the published folder.\n",
    "\n",
    "It uses output generated by the `generate_points_from_JHU` notebook.\n",
    "\n",
    "### Papermill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "data_dir = '/opt/src/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For papermill execution, the pameters are:\n",
    "- data_dir: That data directory to read data from and publish data to.\n",
    "\n",
    "### Google API authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "google_refresh_token = os.environ.get('CHM_GOOGLEAPI_REFRESH_TOKEN', '')\n",
    "google_client_id = os.environ.get('CHM_GOOGLEAPI_CLIENT_ID', '')\n",
    "google_client_secret = os.environ.get('CHM_GOOGLEAPI_CLIENT_SECRET', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the refresh token and client information from the environment. This needs to be set in the environment for local development, and is pulled from GitHub secrets in the data processing GitHub actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not google_refresh_token:\n",
    "    print('NOTE: No google authentication information found; EAC data will not be processed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configures the spreadsheet information for reading the specific sheet filled out by the EAC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EAC_SHEET_ID ='1PVF0mrP9bXgCke-snlHaxaR0Z5CIXwRRFL99HSazDR0'\n",
    "EAC_SHEET_DATA_RANGE = 'Data!A1:U'\n",
    "\n",
    "# Make True to use test rows locally.\n",
    "EAC_USE_TEST_ROWS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point, shape\n",
    "from slugify import slugify\n",
    "\n",
    "from google.oauth2.credentials import Credentials\n",
    "from googleapiclient.discovery import build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eac_countries = [\n",
    "    'Burundi',\n",
    "    'Kenya',\n",
    "    'Rwanda',\n",
    "    'South Sudan',\n",
    "    'Tanzania',\n",
    "    'Uganda'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code(admin0, admin1=None, admin2=None):\n",
    "    \"\"\"Generates a code from JHU data names, which is used to connect\n",
    "    data to feature IDs.\"\"\"\n",
    "    slug_txt = admin0\n",
    "    if admin1 is not None:\n",
    "        slug_txt = \"{} {}\".format(admin1, slug_txt)\n",
    "    if admin2 is not None:\n",
    "        slug_txt = \"{} {}\".format(admin2, slug_txt)\n",
    "    return slugify(slug_txt)\n",
    "\n",
    "def fetch_df(url):\n",
    "    \"\"\"Fetches a Pandas DataFrame from a remote source\"\"\"\n",
    "    r = requests.get(url)\n",
    "    return pd.read_csv(io.BytesIO(r.content))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'case-codes-to-ids-intermidiate.json')) as f:\n",
    "    case_codes_to_ids = json.loads(f.read())\n",
    "with open(os.path.join(data_dir, 'case-codes-to-alpha2.json')) as f:\n",
    "    case_codes_to_alpha2 = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the JHU data from it's source on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_df = fetch_df('https://github.com/CSSEGISandData/COVID-19/raw/master/'\n",
    "                    'csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\n",
    "deaths_df = fetch_df('https://github.com/CSSEGISandData/COVID-19/raw/master/'\n",
    "                     'csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv')\n",
    "recovered_df= fetch_df('https://github.com/CSSEGISandData/COVID-19/raw/master/'\n",
    "                       'csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "US county data. This data only has cases and deaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cases_df = fetch_df('https://github.com/CSSEGISandData/COVID-19/raw/master/'\n",
    "                    'csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv')\n",
    "us_deaths_df = fetch_df('https://github.com/CSSEGISandData/COVID-19/raw/master/'\n",
    "                     'csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_territories = [\n",
    "    'American Samoa',\n",
    "    'Guam',\n",
    "    'Northern Mariana Islands',\n",
    "    'Puerto Rico',\n",
    "    'Virgin Islands'\n",
    "]\n",
    "\n",
    "def filter_us(df):\n",
    "   # Filter out counties that have 0 latest data.\n",
    "    filtered_df = df[df.iloc[:,-1] != 0]\n",
    "    filtered_df = filtered_df[\n",
    "        (filtered_df['Province_State'].isin(us_territories)) |\n",
    "        (\n",
    "            (~filtered_df['Lat'].isnull()) &\n",
    "            (filtered_df['Lat'] != 0.0) &\n",
    "            (~filtered_df['FIPS'].isnull())\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "def reformat_us(df):\n",
    "    columns_to_drop = [\n",
    "        'UID',\n",
    "        'iso2',\n",
    "        'iso3',\n",
    "        'code3',\n",
    "        'FIPS',\n",
    "        'Admin2',\n",
    "        'Province_State',\n",
    "        'Country_Region',\n",
    "        'Combined_Key',\n",
    "        'Long_'\n",
    "    ]\n",
    "    \n",
    "    def set_prov(row):        \n",
    "        prov = row['Province_State']\n",
    "        if row['Admin2'] and (type(row['Admin2']) != float or not np.isnan(row['Admin2'])):\n",
    "            prov = '{} {}'.format(row['Admin2'], prov)\n",
    "        return prov\n",
    "\n",
    "    formatted_df = df.copy()\n",
    "    formatted_df['Province/State'] = formatted_df.apply(set_prov, axis=1)\n",
    "    formatted_df['Country/Region'] = formatted_df['Country_Region']\n",
    "    formatted_df['Long'] = formatted_df['Long_']\n",
    "    formatted_df = formatted_df.drop(columns=columns_to_drop)\n",
    "    return formatted_df\n",
    "    \n",
    "formatted_us_cases_df = reformat_us(filter_us(us_cases_df))\n",
    "formatted_us_deaths_df = reformat_us(filter_us(us_deaths_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map out the formatted dates for each date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_date_columns = ['Province/State', 'Country/Region', 'Lat', 'Long']\n",
    "date_columns = list(set(cases_df.columns) - set(non_date_columns))\n",
    "\n",
    "dates_to_format = {}\n",
    "for d in date_columns:\n",
    "    dt = datetime.strptime(d, '%m/%d/%y')\n",
    "    dates_to_format[d] = dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather JHU data organized by region and date\n",
    "\n",
    "This code gathers the data in an intermediate dictionary, keyed to a region ID that made out of a tuple of the admin0 and admin1 column values. The values represent the total confirmed cases, deaths, and recovered patients for each date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu_case_data = {}\n",
    "jhu_county_case_data = {}\n",
    "alpha2_to_id = {}\n",
    "\n",
    "new_key = max(case_codes_to_ids.values()) + 1\n",
    "\n",
    "def add_new_region(code, body):\n",
    "    global new_key\n",
    "    this_key = new_key\n",
    "    jhu_case_data[new_key] = body\n",
    "    case_codes_to_ids[code] = new_key\n",
    "    new_key += 1 \n",
    "    alpha2_to_id[body['a2']] = this_key\n",
    "    return this_key\n",
    "\n",
    "# Setup multi-national entries\n",
    "# Data points with the 'points' propertyset to None don't display on the map.\n",
    "global_key = add_new_region('global', { 'a2': 'XG', 'map': False, 'dates': {} })\n",
    "eac_key = add_new_region('eac', { 'a2': 'XE', 'map': False, 'dates': {} })\n",
    "\n",
    "# Setup nations with only sub-regions to be totaled up\n",
    "# so that we can display figures at a national level.\n",
    "nations_to_total = {\n",
    "    'Australia': None, \n",
    "    'Canada': None, \n",
    "    'China': None\n",
    "}\n",
    "for nation in nations_to_total:\n",
    "    nations_to_total[nation] = add_new_region(get_code(nation), { \n",
    "        'a2': case_codes_to_alpha2[get_code(nation)],\n",
    "        'map': False,\n",
    "        'dates': {}\n",
    "    })\n",
    "\n",
    "def add_multiregion_cases(key, dt, cases):\n",
    "    if not dt in jhu_case_data[key]['dates']:\n",
    "        jhu_case_data[key]['dates'][dt] = [cases, 0, 0]\n",
    "    else:\n",
    "        (prev_c, _, _) = jhu_case_data[key]['dates'][dt]\n",
    "        jhu_case_data[key]['dates'][dt] = [cases + prev_c, 0, 0]\n",
    "        \n",
    "def add_multiregion_deaths(key, dt, deaths):\n",
    "    (prev_c, total_d, _) = jhu_case_data[key]['dates'][dt]\n",
    "    if total_d is None:\n",
    "        total_d = deaths\n",
    "    else:\n",
    "        total_d += deaths\n",
    "    jhu_case_data[key]['dates'][dt] = [prev_c, total_d, 0]\n",
    "    \n",
    "def add_multiregion_recovered(key, dt, recovered):\n",
    "    (prev_c, prev_d, total_r) = jhu_case_data[key]['dates'][dt]\n",
    "    if total_r is None:\n",
    "        total_r = recovered\n",
    "    else:\n",
    "        total_r += recovered\n",
    "    jhu_case_data[key]['dates'][dt] = [prev_c, prev_d, total_r]\n",
    "    \n",
    "def should_skip(row):\n",
    "    # Skip the cruise ships\n",
    "    lat = row['Lat']\n",
    "    lng = row['Long']\n",
    "    if (lat, lng) == (0, 0):\n",
    "        print('Skipping Cruise Ship {}, {}'.format(row['Province/State'], \n",
    "                                                   row['Country/Region']))\n",
    "        return True\n",
    "    \n",
    "    # Skip unassigned and 'out-of' counties\n",
    "    if type(row['Province/State']) is not float and (\n",
    "        row['Province/State'].startswith('Out of') or\n",
    "        row['Province/State'].startswith('Unassigned')):\n",
    "        print('Skipping odd county data {}, {}'.format(row['Province/State'], \n",
    "                                                       row['Country/Region']))\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def should_map(code):\n",
    "    #return code != 'us'\n",
    "    return True\n",
    "\n",
    "def get_region_info(row):\n",
    "    admin0 = row['Country/Region']\n",
    "    admin1 = row['Province/State']\n",
    "    if type(admin1) is float and np.isnan(admin1):\n",
    "        admin1 = None\n",
    "\n",
    "    code = get_code(admin0, admin1)\n",
    "    country_a2 = case_codes_to_alpha2[code]\n",
    "    region_id = case_codes_to_ids[code]\n",
    "    \n",
    "    is_country = admin1 is None\n",
    "    \n",
    "    return (code, country_a2, region_id, is_country)\n",
    "\n",
    "def process_cases(row, is_county):\n",
    "    if should_skip(row):\n",
    "        return\n",
    "    \n",
    "    data = jhu_county_case_data if is_county else jhu_case_data\n",
    "        \n",
    "    admin0 = row['Country/Region']\n",
    "    code, country_a2, region_id, is_country = get_region_info(row)\n",
    "\n",
    "    data[region_id] = { \n",
    "        'a2': country_a2,\n",
    "        'map': should_map(code),\n",
    "        'dates': {} \n",
    "    }\n",
    "    \n",
    "    if(is_country):\n",
    "        alpha2_to_id[country_a2] = region_id\n",
    "    \n",
    "    for d in date_columns:\n",
    "        cases = row[d]        \n",
    "        dt = dates_to_format[d]\n",
    "        data[region_id]['dates'][dt] = [cases, 0, 0]\n",
    "        \n",
    "        if not is_county:\n",
    "            jhu_case_data[region_id]['dates'][dt] = [cases, 0, 0]\n",
    "            add_multiregion_cases(global_key, dt, cases)\n",
    "\n",
    "            if admin0 in eac_countries:\n",
    "                add_multiregion_cases(eac_key, dt, cases)\n",
    "\n",
    "            if admin0 in nations_to_total:\n",
    "                add_multiregion_cases(nations_to_total[admin0], dt, cases)\n",
    "            \n",
    "def process_deaths(row, is_county):\n",
    "    if should_skip(row):\n",
    "        return\n",
    "    \n",
    "    data = jhu_county_case_data if is_county else jhu_case_data\n",
    "        \n",
    "    admin0 = row['Country/Region']\n",
    "    code, country_a2, region_id, _ = get_region_info(row)\n",
    "\n",
    "    for d in date_columns:\n",
    "        deaths = row[d]\n",
    "        dt = dates_to_format[d]\n",
    "\n",
    "        if region_id not in data:\n",
    "            print(' - Region \"{}\" in deaths but not in cases'.format(code))\n",
    "            continue\n",
    "            \n",
    "        if dt not in data[region_id]['dates']:\n",
    "            print(' - Date {} in deaths but not in cases'.format(dt))\n",
    "            continue\n",
    "            \n",
    "        data[region_id]['dates'][dt][1] = deaths\n",
    "        \n",
    "        if not is_county:\n",
    "            add_multiregion_deaths(global_key, dt, deaths)\n",
    "\n",
    "            if admin0 in eac_countries:\n",
    "                add_multiregion_deaths(eac_key, dt, deaths)\n",
    "\n",
    "            if admin0 in nations_to_total:\n",
    "                add_multiregion_deaths(nations_to_total[admin0], dt, deaths)\n",
    "            \n",
    "def process_recovered(row, is_county):\n",
    "    if should_skip(row):\n",
    "        return\n",
    "    \n",
    "    data = jhu_county_case_data if is_county else jhu_case_data\n",
    "        \n",
    "    admin0 = row['Country/Region']\n",
    "    code, country_a2, region_id, _ = get_region_info(row)\n",
    "\n",
    "    for d in date_columns:\n",
    "        recovered = row[d]\n",
    "        dt = dates_to_format[d]\n",
    "        \n",
    "        if region_id not in data:\n",
    "            print(' - Region \"{}\" in recovered but not in cases'.format(code))\n",
    "            continue\n",
    "            \n",
    "        if dt not in data[region_id]['dates']:\n",
    "            print(' - Date {} in recovered but not in cases'.format(dt))\n",
    "            continue\n",
    "        \n",
    "        # Skip canada as it doesn't match up with the other datasets\n",
    "        if code != 'canada':\n",
    "            data[region_id]['dates'][dt][2] = recovered\n",
    "\n",
    "        if not is_county:\n",
    "            add_multiregion_recovered(global_key, dt, recovered)\n",
    "\n",
    "            if admin0 in eac_countries:\n",
    "                add_multiregion_recovered(eac_key, dt, recovered)\n",
    "\n",
    "            if admin0 in nations_to_total:\n",
    "                add_multiregion_recovered(nations_to_total[admin0], dt, recovered)\n",
    "\n",
    "for _, row in cases_df.iterrows():\n",
    "    process_cases(row, is_county=False)\n",
    "    \n",
    "for _, row in formatted_us_cases_df.iterrows():\n",
    "    process_cases(row, is_county=True)\n",
    "\n",
    "for _, row in deaths_df.iterrows():\n",
    "    process_deaths(row, is_county=False)\n",
    "    \n",
    "for _, row in formatted_us_deaths_df.iterrows():\n",
    "    process_deaths(row, is_county=True)\n",
    "    \n",
    "for _, row in recovered_df.iterrows():\n",
    "    process_recovered(row, is_county=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in nations_to_total:\n",
    "    assert nations_to_total[x] in jhu_case_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max date and latest counts for EAC countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date = sorted(jhu_case_data[eac_key]['dates'], reverse=True)[0]\n",
    "print('MAX DATE: {}'.format(max_date))\n",
    "print('\\nCountry counts:')\n",
    "for x in eac_countries:\n",
    "    print('  {}: {}'.format(x, jhu_case_data[case_codes_to_ids[\n",
    "        get_code(x)]]['dates'][max_date]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer in data from EAC\n",
    "\n",
    "Grab the spreadsheet that's fed from the form being filled out by EAC.\n",
    "\n",
    "Notice that if there ae multiple entries for a date, the last row with that date will take effect. This means that if EAC notices a mistake, they can resubmit for that day and the new data will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_rollup(rollup_id, date, cases, deaths, recovered):\n",
    "    jhu_numbers = jhu_case_data[rollup_id]['dates'].get(date)\n",
    "    if jhu_numbers is not None:\n",
    "        jhu_case_data[rollup_id]['dates'][date] = [\n",
    "            jhu_numbers[0] + cases,\n",
    "            jhu_numbers[1] + deaths,\n",
    "            jhu_numbers[2] + recovered\n",
    "        ]\n",
    "\n",
    "def update_eac_numbers(region_id, date, cases, deaths, recovered):\n",
    "    # Update the specific country and any roll-ups.\n",
    "    # If a roll-up doesn't have that date, don't add it.\n",
    "    jhu_numbers = jhu_case_data[region_id]['dates'].get(date)\n",
    "    \n",
    "    if jhu_numbers is not None:\n",
    "        for rollup_id in [eac_key, global_key]:\n",
    "            update_rollup(\n",
    "                rollup_id,\n",
    "                date,\n",
    "                -jhu_numbers[0],\n",
    "                -jhu_numbers[1],\n",
    "                -jhu_numbers[2]\n",
    "            )\n",
    "    \n",
    "    jhu_case_data[region_id]['dates'][date] = [\n",
    "        eac_numbers['cases'],\n",
    "        eac_numbers['deaths'],\n",
    "        eac_numbers['recovered']\n",
    "    ]\n",
    "    \n",
    "    if jhu_numbers is not None:\n",
    "        for rollup_id in [eac_key, global_key]:\n",
    "            update_rollup(\n",
    "                rollup_id,\n",
    "                date,\n",
    "                cases,\n",
    "                deaths,\n",
    "                recovered\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spreadsheet_data():\n",
    "    creds = Credentials(token=None,\n",
    "                        refresh_token=google_refresh_token,\n",
    "                        token_uri='https://oauth2.googleapis.com/token',\n",
    "                        client_id=google_client_id,\n",
    "                        client_secret=google_client_secret)\n",
    "\n",
    "    service = build('sheets', 'v4', credentials=creds)\n",
    "    sheet = service.spreadsheets()\n",
    "    response = sheet.values().get(\n",
    "        spreadsheetId=EAC_SHEET_ID, \n",
    "        range=EAC_SHEET_DATA_RANGE\n",
    "    ).execute()\n",
    "    result = response.get('values', None)\n",
    "    if result is None:\n",
    "        return None\n",
    "    return pd.DataFrame(result[1:], columns=result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eac_sheet_df = get_spreadsheet_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eac_val_col_mapping = {\n",
    "    'Confirmed Cases': 'cases',\n",
    "    'Recovered Cases': 'recovered',\n",
    "    'Deaths': 'deaths'\n",
    "}\n",
    "\n",
    "eac_country_to_id = {\n",
    "    'Burundi': alpha2_to_id['BI'],\n",
    "    'Kenya': alpha2_to_id['KE'],\n",
    "    'Rwanda': alpha2_to_id['RW'],\n",
    "    'South Sudan': alpha2_to_id['SS'],\n",
    "    'Tanzania': alpha2_to_id['TZ'],\n",
    "    'Uganda': alpha2_to_id['UG']\n",
    "}\n",
    "\n",
    "eac_case_data = {}\n",
    "for region_id in eac_country_to_id.values():\n",
    "    eac_case_data[region_id] = defaultdict(dict)\n",
    "\n",
    "for _, row in eac_sheet_df.iterrows():\n",
    "    dt = datetime.strptime(row['Timestamp of the data'], '%m/%d/%Y')\n",
    "    date = dt.strftime('%Y-%m-%d')\n",
    "    is_test = row['IS_TEST'] == 'Y'\n",
    "    \n",
    "    \n",
    "    for col in eac_sheet_df.columns:\n",
    "        m = re.match('(Confirmed Cases|Recovered Cases|Deaths) \\((.*)\\)', \n",
    "                     col)\n",
    "        if m:\n",
    "            value_type = eac_val_col_mapping[m.group(1)]\n",
    "            country = m.group(2)\n",
    "            region_id = eac_country_to_id[country]\n",
    "            \n",
    "            if not is_test or EAC_USE_TEST_ROWS:\n",
    "                eac_case_data[region_id][date][value_type] = int(row[col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Override or add to JHU data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country_name, region_id in eac_country_to_id.items():\n",
    "    jhu_country_info = jhu_case_data[region_id]\n",
    "    \n",
    "    for date in eac_case_data[region_id]:\n",
    "        eac_numbers = eac_case_data[region_id][date]\n",
    "        update_eac_numbers(\n",
    "            region_id, \n",
    "            date,\n",
    "            eac_numbers['cases'],\n",
    "            eac_numbers['deaths'],\n",
    "            eac_numbers['recovered']\n",
    "        )\n",
    "        print('USING EAC NUMBERS FOR {} ON {}'.format(country_name, date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consolidate data\n",
    "\n",
    "Go through each regions case data, compute active and daily changes, and construct final data json objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_data(data):\n",
    "    result = {}\n",
    "    for region_id, region_data in data.items():\n",
    "        result[region_id] = {}\n",
    "        result[region_id]['a2'] = region_data['a2']\n",
    "        result[region_id]['map'] = region_data['map']\n",
    "\n",
    "        result[region_id]['dates'] = {}\n",
    "\n",
    "        (prev_cases,\n",
    "         prev_deaths,\n",
    "         prev_recovered,\n",
    "         prev_active) = (None,\n",
    "                         None,\n",
    "                         None,\n",
    "                         None)\n",
    "\n",
    "        for date in sorted(region_data['dates']):\n",
    "\n",
    "            (cases,\n",
    "             deaths,\n",
    "             recovered) = region_data['dates'][date]\n",
    "\n",
    "            active = cases - deaths - recovered\n",
    "\n",
    "            (cases_change,\n",
    "             deaths_change,\n",
    "             recovered_change,\n",
    "             active_change) = (None,\n",
    "                               None,\n",
    "                               None,\n",
    "                               None)\n",
    "\n",
    "            if prev_cases is not None:\n",
    "                cases_change = cases - prev_cases\n",
    "                deaths_change = deaths - prev_deaths\n",
    "                recovered_change = recovered - prev_recovered\n",
    "                active_change = active - prev_active\n",
    "\n",
    "\n",
    "            result[region_id]['dates'][date] = {\n",
    "                'cases': cases,\n",
    "                'cases_change': cases_change,\n",
    "                'deaths': deaths,\n",
    "                'deaths_change': deaths_change,\n",
    "                'recovered': recovered,\n",
    "                'recovered_change': recovered_change,\n",
    "                'active': active,\n",
    "                'active_change': active_change,\n",
    "            }\n",
    "\n",
    "            (prev_cases,\n",
    "             prev_deaths,\n",
    "             prev_recovered,\n",
    "             prev_active) = (cases,\n",
    "                             deaths,\n",
    "                             recovered,\n",
    "                             active)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_data = consolidate_data(jhu_case_data)\n",
    "county_case_data = consolidate_data(jhu_county_case_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations = 0\n",
    "for code, region_id in case_codes_to_ids.items():\n",
    "    if region_id not in case_data and region_id not in county_case_data:\n",
    "        violations += 1\n",
    "        print('Region {} does not have data!'.format(code))\n",
    "if violations > 0:\n",
    "    raise Exception('{} regions do not have data!'.format(violations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data used in the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_max_dates = {}\n",
    "all_dates = set([])\n",
    "def max_time(region_id, region_data):\n",
    "    return sorted(map(lambda x: (x, datetime.strptime(x, '%Y-%m-%d')), \n",
    "                  region_data['dates'].keys()), key=lambda x: x[1])[-1][0]\n",
    "   \n",
    "for region_id in case_codes_to_ids.values():\n",
    "    if region_id in case_data:\n",
    "        ids_to_max_dates[region_id] = max_time(region_id, case_data[region_id])\n",
    "        for d in case_data[region_id]['dates'].keys():\n",
    "            all_dates.add(d)\n",
    "    else:  \n",
    "        ids_to_max_dates[region_id] = max_time(region_id, county_case_data[region_id])\n",
    "        for d in county_case_data[region_id]['dates'].keys():\n",
    "            all_dates.add(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_id = alpha2_to_id['XG']\n",
    "case_data[global_id]['dates'][ids_to_max_dates[global_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'case-country-config.json')) as f:\n",
    "    country_list = json.loads(f.read())\n",
    "    \n",
    "    \n",
    "config = {\n",
    "    'maxDates': ids_to_max_dates,\n",
    "    'countries': country_list,\n",
    "    'dates': sorted(list(all_dates)),\n",
    "    'alpha2ToId': alpha2_to_id\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'published/case-data.json'), 'w') as f:\n",
    "    f.write(json.dumps(case_data, sort_keys=True))\n",
    "\n",
    "with open(os.path.join(data_dir, 'published/county-case-data.json'), 'w') as f:\n",
    "    f.write(json.dumps(county_case_data, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'published/case-codes-to-ids.json'), 'w') as f:\n",
    "    f.write(json.dumps(case_codes_to_ids, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'published/case-config.json'), 'w') as f:\n",
    "    f.write(json.dumps(config, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
